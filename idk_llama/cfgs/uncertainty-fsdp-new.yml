base_unit: optimizer-steps
training_goal: 1024
log_interval: -1
eval_interval: 0.05
eval_samples: 10_000
cross_tokenizer_val: True
save_interval: 0.05
warmup_period: 0.05 
block_size: 4096 # changed from 8192, matches original mistral
batch_size: 256 # down from 512 in LeoLM, squeeze in more updates
weight_decay: 0.05
learning_rate: 4e-5 # increased from 2e-5 in LeoLM previously
decay_lr: True # cosine decay
min_lr: 2e-6
beta1: 0.9
beta2: 0.95
grad_clip: 1.0
seed: 42

train_embeddings: True
model_path: mistralai/Mistral-7B-v0.1
out_dir: out/
model_profiling: True
model_profiling_interval: 10

# Efficiency settings
micro_batch_size: 1
fast_model_loading: True
precision: bf16-true
use_anyprecision_adamw: False
activation_checkpointing: True
fsdp_sharding_strategy: "FULL_SHARD"
fsdp_limit_all_gathers: False
fsdp_cpu_offload: False
compile: False
use_additional_flash_attn_kernels: True # RMSNorm kernels lead to graph breaks with `torch.compile` => slower than not using them, yes but mistral needs FA2 from pypi, which doesn't work w/ compile
adamw_foreach: True
workers: 4